{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLE Assessed Coursework 3: Question 2\n",
    "\n",
    "For this assessment, you are expected to complete and submit 4 notebook files.  There is 1 notebook file for each question (to speed up load times).  This is notebook 2 out of 4.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateno=203068 #this MUST be updated to your candidate number so that you get a unique data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminary imports\n",
    "import sys\n",
    "\n",
    "sys.path.append('')\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import zip_longest\n",
    "from sussex_nltk.corpus_readers import ReutersCorpusReader\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "\n",
    "def normalise(tokenlist):\n",
    "    tokenlist=[token.lower() for token in tokenlist]\n",
    "    tokenlist=[\"NUM\" if token.isdigit() else token for token in tokenlist]\n",
    "    tokenlist=[\"Nth\" if (token.endswith((\"nd\",\"st\",\"th\")) and token[:-2].isdigit()) else token for token in tokenlist]\n",
    "    tokenlist=[\"NUM\" if re.search(\"^[+-]?[0-9]+\\.[0-9]\",token) else token for token in tokenlist]\n",
    "    return tokenlist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Distributional Semantics (25 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcr = ReutersCorpusReader().finance()\n",
    "rcr.enumerate_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(candidateno)  \n",
    "samplesize=2000\n",
    "iterations =100\n",
    "sentences=[]\n",
    "for i in range(0,iterations):\n",
    "    sentences+=[normalise(sent) for sent in rcr.sample_sents(samplesize=samplesize)]\n",
    "    print(\"Completed {}%\".format(i))\n",
    "print(\"Completed 100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(sentences,window=1):\n",
    "    mydict={}\n",
    "    for sentence in sentences:\n",
    "        for i,token in enumerate(sentence):\n",
    "            current=mydict.get(token,{})\n",
    "            features=sentence[max(0,i-window):i]+sentence[i+1:i+window+1]\n",
    "            for feature in features:\n",
    "                current[feature]=current.get(feature,0)+1\n",
    "            mydict[token]=current\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) **Run** `generate_features(sentences[:5])`.  With reference to the code and the specific examples, **explain** how the output was generated. \\[4 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Write code and **find** the 100 most frequently occurring words that\n",
    "* are in your sample; AND\n",
    "* have at least one noun sense according to WordNet\n",
    "\\[ 4 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Write code to create distributional vector representations of each word in the corpus with a parameter to specify context window size.   \\[5 marks \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Plan and carry out an investigation into the correlation between semantic similarity according to the WordNet path similarity measure and distributional similarity with different context window sizes.  You should make sure that you include a graph of how correlation varies with context window size and that you discuss your results.  \\[12 marks\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
